{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3dac5d-d043-432d-96f2-ce0df4b219da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a23c47a-b5c8-464d-81b6-eca54690a51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://766eb1b9a2f1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Process json from kafka</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7dd6a1799330>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Process json from kafka\")\n",
    "    .config(\"spark.sql.streaming.stopGracefullyOnShutdown\", True)\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4)\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff1594e-350d-4db6-b4a5-3e23bef80216",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderjson_df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"ed-kafka:29092\")\n",
    "    .option(\"subscribe\", \"custords\")\n",
    "    .option(\"startingOffsets\", \"earliest\") #earliest/latest\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f5966c4-4c6e-47d5-a930-afafaf5e96e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderjson_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8ae30bd-ec45-4482-9a3b-2f30f57b5d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#orderjson_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73d40537-53ed-4191-8b11-c0eae98db90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "stringjson_df = orderjson_df.withColumn(\"value\", expr(\"cast(value as string)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "801fb3b9-73bb-48b0-b4c7-411c0ee1b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stringjson_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fab38abf-b978-4694-810b-f220c389f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = \"\"\"\n",
    "struct<\n",
    "  event_id:string,\n",
    "  event_type:string,\n",
    "  event_timestamp:string,\n",
    "  order:struct<\n",
    "    order_id:string,\n",
    "    customer_id:string,\n",
    "    currency:string,\n",
    "    product_id:string,\n",
    "    product_name:string,\n",
    "    category:string,\n",
    "    quantity:int,\n",
    "    unit_price:int,\n",
    "    total_price:int\n",
    "  >,\n",
    "  payment:struct<\n",
    "    payment_method:string,\n",
    "    status:string,\n",
    "    transaction_id:string\n",
    "  >,\n",
    "  customer_context:struct<\n",
    "    device:string,\n",
    "    location:string,\n",
    "    ip_address:string,\n",
    "    session_id:string\n",
    "  >\n",
    ">\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import from_json\n",
    "df_with_schema = stringjson_df.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), json_schema).alias(\"data\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7a55b52-e4a6-4d6a-876f-4ea8b1a66abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_with_schema.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e28aa890-df3b-4b84-8825-05ef067670c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5441f9fc-6988-407c-8208-879422f5f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bronze_df = df_with_schema.select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be97a60e-9c95-4bc0-a612-30ff19cb3004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bronze_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7a7b15a-89ac-48ae-bbfa-890e28f7d029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "Bronze_df_final = Bronze_df.select(\"event_id\", \"event_type\", \"event_timestamp\", \"order.*\", \"payment.*\", \"customer_context.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c6e6d37-3545-47fd-bad5-c4f30348547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bronze_df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec9d19e8-b38a-4d9c-af64-6f95e7e43253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running in once/available now and processingTime mode\n",
    "#write the output to sink to check output\n",
    "\n",
    "(\n",
    "    Bronze_df_final\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(once=True) #processingTime=\"10 seconds\"\n",
    "    .option(\"checkpointLocation\", \"checkpoint_dir_custords_1\")\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9efe5a7-8fca-431c-a68e-b88c008f56ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/local/lib/python3.10/socket.py\", line 717, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#running in continuous mode\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#write the output to memory sink to check output\u001b[39;00m\n\u001b[1;32m      4\u001b[0m (\n\u001b[1;32m      5\u001b[0m     \u001b[43mdf_expanded_2\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka_table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontinuous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m10 seconds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoint_dir_custords_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m/spark/python/pyspark/sql/streaming.py:107\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#running in continuous mode\n",
    "#write the output to memory sink to check output\n",
    "\n",
    "(\n",
    "    Bronze_df_final\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"kafka_table\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(continuous=\"10 seconds\")\n",
    "    .option(\"checkpointLocation\", \"checkpoint_dir_custords_2\")\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    "^)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80b1da28-e046-4f64-8904-61277247a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to parquet\n",
    "\n",
    "bronze_data_write = (\n",
    "    Bronze_df_final.writeStream\n",
    "    .format(\"parquet\")     # Delta if available\n",
    "    .option(\"checkpointLocation\", \"chk/bronze\")\n",
    "    .option(\"path\", \"data/bronze/orders\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "634c7782-106c-4204-9670-a739056ad2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_timestamp: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- unit_price: integer (nullable = true)\n",
      " |-- total_price: integer (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- device: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bronze_df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7916fe3f-b717-41c5-b3e2-c265418b0454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "silver_df = (\n",
    "    Bronze_df_final\n",
    "    .withColumn(\"event_ts\", to_timestamp(\"event_timestamp\"))\n",
    "    .dropDuplicates([\"event_id\"])\n",
    "    .filter(col(\"status\") == \"PAID\")\n",
    "    .select(\n",
    "        \"event_id\",\n",
    "        \"event_type\",\n",
    "        \"event_ts\",\n",
    "        \"order_id\",\n",
    "        \"customer_id\",\n",
    "        \"product_id\",\n",
    "        \"product_name\",\n",
    "        \"category\",\n",
    "        \"quantity\",\n",
    "        \"unit_price\",\n",
    "        \"total_price\",\n",
    "        \"payment_method\",\n",
    "        \"location\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "silver_write = (\n",
    "    silver_df.writeStream\n",
    "    .format(\"parquet\")\n",
    "    .option(\"checkpointLocation\", \"chk/silver\")\n",
    "    .option(\"path\", \"data/silver/orders\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a908b-7a28-4cf8-b1f9-aa99cde5d1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9341659-1232-46d4-8b68-1f807ab70bed",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: kafka_table; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [kafka_table], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# view data in memory sink\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mselect * from kafka_table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/spark/python/pyspark/sql/session.py:1034\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     sqlQuery \u001b[38;5;241m=\u001b[39m formatter\u001b[38;5;241m.\u001b[39mformat(sqlQuery, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: kafka_table; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [kafka_table], [], false\n"
     ]
    }
   ],
   "source": [
    "# view data in memory sink\n",
    "\n",
    "spark.sql(\"select * from kafka_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d59b3-6099-4852-b911-b9153172899e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91461d60-8731-4d51-9be1-93d0715884a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_kafka_batch = spark.sql(\"select * from kafka_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c440162-06c8-46a2-80d3-0d4edb44aecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_timestamp: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- unit_price: integer (nullable = true)\n",
      " |-- total_price: integer (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- device: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_kafka_batch.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6c05066-96bd-4d94-a200-8f319d86d1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|       product_name|\n",
      "+-------------------+\n",
      "|Mechanical Keyboard|\n",
      "|       Office Chair|\n",
      "|      Running Shoes|\n",
      "|     Wireless Mouse|\n",
      "|         Coffee Mug|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_kafka_batch.select(\"product_name\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50dfa5c4-1ee2-440c-a62a-60b69bb5ceff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|       product_name|total_qty|\n",
      "+-------------------+---------+\n",
      "|     Wireless Mouse|     5801|\n",
      "|      Running Shoes|     5745|\n",
      "|       Office Chair|     5676|\n",
      "|         Coffee Mug|     5650|\n",
      "|Mechanical Keyboard|     5626|\n",
      "+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check the best sellling products by number of orders\n",
    "\n",
    "df_best_products = df_from_kafka_batch.selectExpr(\"product_name\", \"quantity\").groupBy(\"product_name\").agg(sum(\"quantity\").alias(\"total_qty\")).orderBy(col(\"total_qty\").desc())\n",
    "\n",
    "df_best_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d22ad58-be65-42d0-ba71-b47a5dcfea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the customers with the most spend\n",
    "\n",
    "from pyspark.sql.functions import col, sum, asc, desc\n",
    "Gold_df_1 = silver_df.selectExpr(\"customer_id\", \"cast((unit_price * quantity) as double) as order_price\")\\\n",
    ".groupBy(\"customer_id\").agg(sum(\"order_price\").alias(\"total_order_amt\")).orderBy(col(\"total_order_amt\").desc()).limit(5)\n",
    "                                                                                                                     \n",
    "#Gold_df_1.show()\n",
    "                                                                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f65cc6d7-3797-421d-ab7c-2b78e335464c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_ts: timestamp (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- unit_price: integer (nullable = true)\n",
      " |-- total_price: integer (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "silver_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94e94f7a-fa0e-4a79-9e0c-f01c6dd67f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, sum\n",
    "\n",
    "gold_df_2 = (\n",
    "    silver_df\n",
    "    .withWatermark(\"event_ts\", \"10 minutes\")\n",
    "    .groupBy(\n",
    "        window(\"event_ts\", \"10 minutes\"),\n",
    "        col(\"category\")\n",
    "    )\n",
    "    .agg(\n",
    "        sum(\"total_price\").alias(\"revenue\")\n",
    "    )\n",
    ")\n",
    "\n",
    "gold_write = (\n",
    "    gold_df_2.writeStream\n",
    "    .format(\"parquet\")\n",
    "    .option(\"checkpointLocation\", \"chk/gold\")\n",
    "    .option(\"path\", \"data/gold/revenue_by_category\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6979470a-4395-4c7a-b6d8-51d69ef13f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
